{
    "sourceFile": "tsgm/metrics/metrics.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1715324432201,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1715324432201,
            "name": "Commit-0",
            "content": "import abc\nimport antropy\nimport typing as T\nimport logging\nimport numpy as np\nimport itertools\nimport sklearn\nimport scipy\nfrom tqdm import tqdm\nfrom tensorflow.python.types.core import TensorLike\n\nimport tsgm\n\nlogger = logging.getLogger('utils')\nlogger.setLevel(logging.DEBUG)\n\n\nDEFAULT_SPLIT_STRATEGY = sklearn.model_selection.KFold(\n    n_splits=3, random_state=42, shuffle=True)\n\n\ndef _dataset_or_tensor_to_tensor(D1: tsgm.dataset.DatasetOrTensor) -> tsgm.types.Tensor:\n    if isinstance(D1, tsgm.dataset.Dataset):\n        return D1.X\n    else:\n        return D1\n\n\nclass Metric(abc.ABC):\n    @abc.abstractmethod\n    def __call__(self, *args, **kwargs) -> float:\n        pass\n\n\nclass DistanceMetric(Metric):\n    \"\"\"\n    Metric that measures similarity between synthetic and real time series\n    \"\"\"\n    def __init__(self, statistics: list, discrepancy: T.Callable) -> None:\n        \"\"\"\n        :param statistics: A list of summary statistics (callable)\n        :type statistics: list\n        :param discrepancy: Discrepancy function, measures the distance between the vectors of summary statistics.\n        :type discrepancy: typing.Callable\n        \"\"\"\n        self._statistics = statistics\n        self._discrepancy = discrepancy\n\n    def stats(self, X: tsgm.types.Tensor) -> tsgm.types.Tensor:\n        \"\"\"\n        :param X: A time series dataset.\n        :type X: tsgm.types.Tensor.\n\n        :returns: a tensor with calculated summary statistics.\n        \"\"\"\n        return np.array(list(itertools.chain.from_iterable(s(X) for s in self._statistics))) if X is not None else None\n\n    def discrepancy(self, stats1: tsgm.types.Tensor, stats2: tsgm.types.Tensor) -> float:\n        \"\"\"\n        :param stats1: A vector of summary statistics.\n        :type stats1: tsgm.types.Tensor.\n        :param stats2: A vector of summary statistics.\n        :type stats2: tsgm.types.Tensor.\n\n        :returns: the distance between two vectors calculated by self._discrepancy.\n        \"\"\"\n        return self._discrepancy(stats1, stats2)\n\n    def __call__(self, D1: tsgm.dataset.DatasetOrTensor, D2: tsgm.dataset.DatasetOrTensor) -> float:\n        \"\"\"\n        :param D1: A time series dataset.\n        :type D1: tsgm.dataset.DatasetOrTensor.\n        :param D2: A time series dataset.\n        :type D2: tsgm.dataset.DatasetOrTensor.\n\n        :returns: similarity metric between D1 & D2.\n        \"\"\"\n\n        if isinstance(D1, tsgm.dataset.Dataset) and isinstance(D2, tsgm.dataset.Dataset):\n            X1, X2 = D1.Xy_concat, D2.Xy_concat\n        else:\n            X1, X2 = D1, D2\n\n        stats1, stats2 = self.stats(X1), self.stats(X2)\n\n        return self.discrepancy(stats1, stats2)\n\n\nclass ConsistencyMetric(Metric):\n    \"\"\"\n    Predictive consistency metric measures whether a set of evaluators yield consistent results on real and synthetic data.\n    \"\"\"\n    def __init__(self, evaluators: T.List) -> None:\n        \"\"\"\n        :param evaluators: A list of evaluators (each item should implement method `.evaluate(D)`)\n        :type evaluators: list\n        \"\"\"\n        self._evaluators = evaluators\n\n    def _apply_models(self, D: tsgm.dataset.DatasetOrTensor, D_test: tsgm.dataset.DatasetOrTensor) -> T.List:\n        return [e.evaluate(D, D_test) for e in self._evaluators]\n\n    def __call__(self, D1: tsgm.dataset.DatasetOrTensor, D2: tsgm.dataset.DatasetOrTensor, D_test: tsgm.dataset.DatasetOrTensor) -> float:\n        \"\"\"\n        :param D1: A time series dataset.\n        :type D1: tsgm.dataset.DatasetOrTensor.\n        :param D2: A time series dataset.\n        :type D2: tsgm.dataset.DatasetOrTensor.\n\n        :returns: consistency metric between D1 & D2.\n        \"\"\"\n        evaluations1 = self._apply_models(D1, D_test)\n        evaluations2 = self._apply_models(D2, D_test)\n        consistencies_cnt = 0\n        n_evals = len(evaluations1)\n        for i1 in tqdm(range(n_evals)):\n            for i2 in range(i1 + 1, n_evals):\n                if evaluations1[i1] > evaluations1[i2] and evaluations2[i1] > evaluations2[i2] or \\\n                        evaluations1[i1] < evaluations1[i2] and evaluations2[i1] < evaluations2[i2] or \\\n                        evaluations1[i1] == evaluations1[i2] and evaluations2[i1] == evaluations2[i2]:\n                    consistencies_cnt += 1\n\n        total_pairs = n_evals * (n_evals - 1) / 2.0\n        return consistencies_cnt / total_pairs\n\n\nclass BaseDownstreamEvaluator(abc.ABC):\n    def evaluate(self, *args, **kwargs):\n        pass\n\n\nclass DownstreamPerformanceMetric(Metric):\n    \"\"\"\n    The downstream performance metric evaluates the performance of a model on a downstream task.\n    It returns performance gains achieved with the addition of synthetic data.\n    \"\"\"\n    def __init__(self, evaluator: BaseDownstreamEvaluator) -> None:\n        \"\"\"\n        :param evaluator: An evaluator,  should implement method `.evaluate(D)`\n        :type evaluator: BaseDownstreamEvaluator\n        \"\"\"\n        self._evaluator = evaluator\n\n    def __call__(self, D1: tsgm.dataset.DatasetOrTensor, D2: tsgm.dataset.DatasetOrTensor, D_test: T.Optional[tsgm.dataset.DatasetOrTensor], return_std: bool = False) -> float:\n        \"\"\"\n        :param D1: A time series dataset.\n        :type D1: tsgm.dataset.DatasetOrTensor.\n        :param D2: A time series dataset.\n        :type D2: tsgm.dataset.DatasetOrTensor.\n\n        :returns: downstream performance metric between D1 & D2.\n        \"\"\"\n        if isinstance(D1, tsgm.dataset.Dataset) and isinstance(D2, tsgm.dataset.Dataset):\n            D1D2 = D1 | D2\n        else:\n            if isinstance(D1, tsgm.dataset.Dataset):\n                D1D2 = np.concatenate((D1.X, D2))\n            elif isinstance(D2, tsgm.dataset.Dataset):\n                D1D2 = np.concatenate((D1, D2.X))\n            else:\n                D1D2 = np.concatenate((D1, D2))\n        evaluations1 = self._evaluator.evaluate(D1, D_test)\n        evaluations2 = self._evaluator.evaluate(D1D2, D_test)\n        if return_std:\n            diff = evaluations2 - evaluations1\n            return np.mean(diff), np.std(diff)\n        else:\n            return np.mean(evaluations2 - evaluations1)\n\n\nclass PrivacyMembershipInferenceMetric(Metric):\n    \"\"\"\n    The metric that measures the possibility of membership inference attacks.\n    \"\"\"\n    def __init__(self, attacker: T.Any, metric: T.Optional[T.Callable] = None) -> None:\n        \"\"\"\n        :param attacker: An attacker, one class classififier (OCC) that implements methods `.fit` and `.predict`\n        :type attacker: typing.Any\n        :param metric: Measures quality of attacker (precision by default)\n        :type attacker: typing.Callable\n        \"\"\"\n        self._attacker = attacker\n        self._metric = metric or sklearn.metrics.precision_score\n\n    def __call__(self, d_tr: tsgm.dataset.Dataset, d_syn: tsgm.dataset.Dataset, d_test: tsgm.dataset.Dataset) -> float:\n        \"\"\"\n        :param d_tr: Training dataset (the dataset that was used to produce `d_dyn`).\n        :type d_tr: tsgm.dataset.DatasetOrTensor.\n        :param d_syn: Training dataset (the dataset that was used to produce `d_dyn`).\n        :type d_syn: tsgm.dataset.DatasetOrTensor.\n        :param d_test: Training dataset (the dataset that was used to produce `d_dyn`).\n        :type d_test: tsgm.dataset.DatasetOrTensor.\n\n        :returns: how well the attacker can distinguish `d_tr` & `d_test` when it is trained on `d_syn`.\n        \"\"\"\n        self._attacker.fit(d_syn.Xy_concat)\n        labels = self._attacker.predict((d_tr + d_test).Xy_concat)\n        correct_labels = [1] * len(d_tr) + [-1] * len(d_test)\n        return 1 - self._metric(labels, correct_labels)\n\n\nclass MMDMetric(Metric):\n    \"\"\"\n    This metric calculated MMD between real and synthetic samples\n\n    Args:\n        d (tsgm.dataset.DatasetOrTensor): The input dataset or tensor.\n\n    Returns:\n        float: The computed spectral entropy.\n\n    Example:\n        >>> metric = MMDMetric(kernel)\n        >>> dataset, synth_dataset = tsgm.dataset.Dataset(...), tsgm.dataset.Dataset(...)\n        >>> result = metric(dataset)\n        >>> print(result)\n    \"\"\"\n    def __init__(self, kernel: T.Callable = tsgm.utils.mmd.exp_quad_kernel) -> None:\n        self.kernel = kernel\n\n    def __call__(self, D1: tsgm.dataset.DatasetOrTensor, D2: tsgm.dataset.DatasetOrTensor) -> float:\n        if isinstance(D1, tsgm.dataset.Dataset) and D1.y is not None or isinstance(D2, tsgm.dataset.Dataset) and D2.y is not None:\n            logger.warning(\"It is currently impossible to run MMD for labeled time series. Labels will be ignored!\")\n        X1, X2 = _dataset_or_tensor_to_tensor(D1), _dataset_or_tensor_to_tensor(D2)\n        return tsgm.utils.mmd.MMD(X1, X2, kernel=self.kernel)\n\n\nclass DiscriminativeMetric(Metric):\n    \"\"\"\n    The DiscriminativeMetric measures the discriminative performance of a model in distinguishing\n    between synthetic and real datasets.\n\n    This metric evaluates a discriminative model by training it on a combination of synthetic\n    and real datasets and assessing its performance on a test set.\n\n    :param d_hist: Real dataset.\n    :type d_hist: tsgm.dataset.DatasetOrTensor\n    :param d_syn: Synthetic dataset.\n    :type d_syn: tsgm.dataset.DatasetOrTensor\n    :param model: Discriminative model to be evaluated.\n    :type model: T.Callable\n    :param test_size: Proportion of the dataset to include in the test split\n                     or the absolute number of test samples.\n    :type test_size: T.Union[float, int]\n    :param n_epochs: Number of training epochs for the model.\n    :type n_epochs: int\n    :param metric: Optional evaluation metric to use (default: accuracy).\n    :type metric: T.Optional[T.Callable]\n    :param random_seed: Optional random seed for reproducibility.\n    :type random_seed: T.Optional[int]\n\n    :return: Discriminative performance metric.\n    :rtype: float\n\n    Example:\n    --------\n    >>> from my_module import DiscriminativeMetric, MyDiscriminativeModel\n    >>> import tsgm.dataset\n    >>> import numpy as np\n    >>> import sklearn\n    >>>\n    >>> # Create real and synthetic datasets\n    >>> real_dataset = tsgm.dataset.Dataset(...)  # Replace ... with appropriate arguments\n    >>> synthetic_dataset = tsgm.dataset.Dataset(...)  # Replace ... with appropriate arguments\n    >>>\n    >>> # Create a discriminative model\n    >>> model = MyDiscriminativeModel()  # Replace with the actual discriminative model class\n    >>>\n    >>> # Create and use the DiscriminativeMetric\n    >>> metric = DiscriminativeMetric()\n    >>> result = metric(real_dataset, synthetic_dataset, model, test_size=0.2, n_epochs=10)\n    >>> print(result)\n    \"\"\"\n    def __call__(self, d_hist: tsgm.dataset.DatasetOrTensor, d_syn: tsgm.dataset.DatasetOrTensor, model: T.Callable, test_size: T.Union[float, int], n_epochs: int, metric: T.Optional[T.Callable] = None, random_seed: T.Optional[int] = None) -> float:\n        X_hist, X_syn = _dataset_or_tensor_to_tensor(d_hist), _dataset_or_tensor_to_tensor(d_syn)\n        X_all, y_all = np.concatenate([X_hist, X_syn]), np.concatenate([[1] * len(d_hist), [0] * len(d_syn)])\n        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_all, y_all, test_size=test_size, random_state=random_seed)\n        model.fit(X_train, y_train, epochs=n_epochs)\n        pred = model.predict(X_test)\n        # check the shape, 1D array or N-D arrary\n        if len(pred.shape) == 1:  # binary classification with sigmoid activation\n            y_pred = (pred > 0.5).astype(int)\n        else:  # multiple classification with softmax activation\n            y_pred = np.argmax(pred, axis=-1).astype(int)\n        if metric is None:\n            return sklearn.metrics.accuracy_score(y_test, y_pred)\n        else:\n            return metric(y_test, y_pred)\n\n\ndef _spectral_entropy_per_feature(X: TensorLike) -> TensorLike:\n    return antropy.spectral_entropy(X.ravel(), sf=1, method='welch', normalize=True)\n\n\ndef _spectral_entropy_per_sample(X: TensorLike) -> TensorLike:\n    if len(X.shape) == 1:\n        X = X[:, None]\n    return np.apply_along_axis(_spectral_entropy_per_feature, 0, X)\n\n\ndef _spectral_entropy_sum(X: TensorLike) -> TensorLike:\n    return np.apply_along_axis(_spectral_entropy_per_sample, 1, X)\n\n\nclass EntropyMetric(Metric):\n    \"\"\"\n    Calculates the spectral entropy of a dataset or tensor.\n\n    This metric measures the randomness or disorder in a dataset or tensor\n    using spectral entropy, which is a measure of the distribution of energy\n    in the frequency domain.\n\n    Args:\n        d (tsgm.dataset.DatasetOrTensor): The input dataset or tensor.\n\n    Returns:\n        float: The computed spectral entropy.\n\n    Example:\n        >>> metric = EntropyMetric()\n        >>> dataset = tsgm.dataset.Dataset(...)\n        >>> result = metric(dataset)\n        >>> print(result)\n    \"\"\"\n    def __call__(self, d: tsgm.dataset.DatasetOrTensor) -> float:\n        \"\"\"\n        Calculate the spectral entropy of the input dataset or tensor.\n\n        Args:\n            d (tsgm.dataset.DatasetOrTensor): The input dataset or tensor.\n\n        Returns:\n            float: The computed spectral entropy.\n        \"\"\"\n        X = _dataset_or_tensor_to_tensor(d)\n        return np.sum(_spectral_entropy_sum(X), axis=None)\n\n\nclass DemographicParityMetric(Metric):\n    \"\"\"\n    Measuring demographic parity between two datasets.\n\n    This metric assesses the disparity in the distributions of a target variable among different groups in two datasets.\n    By default, it uses the Kolmogorov-Smirnov statistic to quantify the maximum vertical deviation between the cumulative distribution functions\n    of the target variable for the historical and synthetic data within each group.\n\n    Args:\n        d_hist (tsgm.dataset.DatasetOrTensor): The historical input dataset or tensor.\n        groups_hist (TensorLike): The group assignments for the historical data.\n        d_synth (tsgm.dataset.DatasetOrTensor): The synthetic input dataset or tensor.\n        groups_synth (TensorLike): The group assignments for the synthetic data.\n        metric (callable, optional): The metric used to compare the target variable distributions within each group.\n            Default is the Kolmogorov-Smirnov statistic.\n\n    Returns:\n        dict: A dictionary mapping each group to the computed demographic parity metric.\n\n    Example:\n        >>> metric = DemographicParityMetric()\n        >>> dataset_hist = tsgm.dataset.Dataset(...)\n        >>> dataset_synth = tsgm.dataset.Dataset(...)\n        >>> groups_hist = [0, 1, 0, 1, 1, 0]\n        >>> groups_synth = [1, 1, 0, 0, 0, 1]\n        >>> result = metric(dataset_hist, groups_hist, dataset_synth, groups_synth)\n        >>> print(result)\n    \"\"\"\n\n    _DEFAULT_KS_METRIC = lambda data1, data2: scipy.stats.ks_2samp(data1, data2).statistic  # noqa: E731\n\n    def __call__(self, d_hist: tsgm.dataset.DatasetOrTensor, groups_hist: TensorLike, d_synth: tsgm.dataset.DatasetOrTensor, groups_synth: TensorLike, metric: T.Callable = _DEFAULT_KS_METRIC) -> T.Dict:\n        \"\"\"\n        Calculate the demographic parity metric for the input datasets.\n\n        Args:\n            d_hist (tsgm.dataset.DatasetOrTensor): The historical input dataset or tensor.\n            groups_hist (TensorLike): The group assignments for the historical data.\n            d_synth (tsgm.dataset.DatasetOrTensor): The synthetic input dataset or tensor.\n            groups_synth (TensorLike): The group assignments for the synthetic data.\n            metric (callable, optional): The metric used to compare the target variable distributions within each group.\n                Default is the Kolmogorov-Smirnov statistic.\n\n        Returns:\n            dict: A dictionary mapping each group to the computed demographic parity metric.\n        \"\"\"\n\n        y_hist, y_synth = d_hist.y, d_synth.y\n\n        unique_groups_hist, unique_groups_synth = set(groups_hist), set(groups_synth)\n        all_groups = unique_groups_hist | unique_groups_synth\n        if len(all_groups) > len(unique_groups_hist) or len(all_groups) > len(unique_groups_synth):\n            logger.warning(\"Groups in historical and synthetic data are not entirely identical.\")\n\n        result = {}\n        for g in all_groups:\n            y_g_hist, y_g_synth = y_hist[groups_hist == g], y_synth[groups_synth == g]\n            if not len(y_g_synth):\n                result[g] = np.inf\n            elif not len(y_g_hist):\n                result[g] = -np.inf\n            else:\n                result[g] = metric(y_g_hist, y_g_synth)\n        return result\n"
        }
    ]
}